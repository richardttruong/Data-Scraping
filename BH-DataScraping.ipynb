{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc26b69-0d91-4a27-8b9c-aa2f0fca75a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.13/site-packages (4.13.5)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.13/site-packages (5.3.0)\n",
      "Requirement already satisfied: openpyxl in /opt/anaconda3/lib/python3.13/site-packages (3.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.13/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et_xmlfile in /opt/anaconda3/lib/python3.13/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas lxml openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c015347-9a49-4657-80ba-fb0ffd16c8e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ Fetching SEC JSON index...\n",
      "2Ô∏è‚É£ Filtering first 15 valid 13F-HR filings (excluding amendments)...\n",
      "   ‚úì 13F-HR | 2025-11-14 | 0001193125-25-282901\n",
      "   ‚úì 13F-HR | 2025-08-14 | 0000950123-25-008343\n",
      "   ‚úì 13F-HR | 2025-05-15 | 0000950123-25-005701\n",
      "   ‚úì 13F-HR | 2025-02-14 | 0000950123-25-002701\n",
      "   ‚úì 13F-HR | 2024-11-14 | 0000950123-24-011775\n",
      "   ‚úì 13F-HR | 2024-08-14 | 0000950123-24-008740\n",
      "   ‚úì 13F-HR | 2024-05-15 | 0000950123-24-005622\n",
      "   ‚úì 13F-HR | 2024-02-14 | 0000950123-24-002518\n",
      "   ‚úì 13F-HR | 2023-11-14 | 0000950123-23-010898\n",
      "   ‚úì 13F-HR | 2023-08-14 | 0000950123-23-008074\n",
      "   ‚úì 13F-HR | 2023-05-15 | 0000950123-23-005270\n",
      "   ‚úì 13F-HR | 2023-02-14 | 0000950123-23-002585\n",
      "   ‚úì 13F-HR | 2022-11-14 | 0000950123-22-012275\n",
      "   ‚úì 13F-HR | 2022-08-15 | 0000950123-22-009450\n",
      "   ‚úì 13F-HR | 2022-05-16 | 0000950123-22-006442\n",
      "\n",
      "   Found 15 filings\n",
      "\n",
      "============================================================\n",
      "Processing 1/15 ‚Äî Filing Date: 2025-11-14\n",
      "Accession: 0001193125-25-282901\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000119312525282901/0001193125-25-282901-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000119312525282901/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000119312525282901/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='46994.html'\n",
      "         ‚Üí Link text: '46994.html', href: '/Archives/edgar/data/1067983/000119312525282901/xslForm13F_X02/46994.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 46994.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000119312525282901/xslForm13F_X02/46994.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Extracted table with 118 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2025_11_14\n",
      "\n",
      "============================================================\n",
      "Processing 2/15 ‚Äî Filing Date: 2025-08-14\n",
      "Accession: 0000950123-25-008343\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012325008343/0000950123-25-008343-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012325008343/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012325008343/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='43977.html'\n",
      "         ‚Üí Link text: '43977.html', href: '/Archives/edgar/data/1067983/000095012325008343/xslForm13F_X02/43977.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 43977.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012325008343/xslForm13F_X02/43977.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 117 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2025_08_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing 3/15 ‚Äî Filing Date: 2025-05-15\n",
      "Accession: 0000950123-25-005701\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012325005701/0000950123-25-005701-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012325005701/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012325005701/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='form13fInfoTable.html'\n",
      "         ‚Üí Link text: 'form13fInfoTable.html', href: '/Archives/edgar/data/1067983/000095012325005701/xslForm13F_X02/form13fInfoTable.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: form13fInfoTable.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012325005701/xslForm13F_X02/form13fInfoTable.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 113 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2025_05_15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing 4/15 ‚Äî Filing Date: 2025-02-14\n",
      "Accession: 0000950123-25-002701\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012325002701/0000950123-25-002701-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012325002701/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012325002701/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='39042.html'\n",
      "         ‚Üí Link text: '39042.html', href: '/Archives/edgar/data/1067983/000095012325002701/xslForm13F_X02/39042.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 39042.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012325002701/xslForm13F_X02/39042.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 115 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2025_02_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing 5/15 ‚Äî Filing Date: 2024-11-14\n",
      "Accession: 0000950123-24-011775\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012324011775/0000950123-24-011775-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012324011775/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012324011775/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='36917.html'\n",
      "         ‚Üí Link text: '36917.html', href: '/Archives/edgar/data/1067983/000095012324011775/xslForm13F_X02/36917.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 36917.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012324011775/xslForm13F_X02/36917.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Extracted table with 124 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2024_11_14\n",
      "\n",
      "============================================================\n",
      "Processing 6/15 ‚Äî Filing Date: 2024-08-14\n",
      "Accession: 0000950123-24-008740\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012324008740/0000950123-24-008740-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012324008740/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012324008740/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='34725.html'\n",
      "         ‚Üí Link text: '34725.html', href: '/Archives/edgar/data/1067983/000095012324008740/xslForm13F_X02/34725.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 34725.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012324008740/xslForm13F_X02/34725.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 132 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2024_08_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing 7/15 ‚Äî Filing Date: 2024-05-15\n",
      "Accession: 0000950123-24-005622\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012324005622/0000950123-24-005622-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012324005622/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012324005622/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='32398.html'\n",
      "         ‚Üí Link text: '32398.html', href: '/Archives/edgar/data/1067983/000095012324005622/xslForm13F_X02/32398.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 32398.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012324005622/xslForm13F_X02/32398.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 136 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2024_05_15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing 8/15 ‚Äî Filing Date: 2024-02-14\n",
      "Accession: 0000950123-24-002518\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012324002518/0000950123-24-002518-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012324002518/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012324002518/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='30197.html'\n",
      "         ‚Üí Link text: '30197.html', href: '/Archives/edgar/data/1067983/000095012324002518/xslForm13F_X02/30197.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 30197.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012324002518/xslForm13F_X02/30197.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 141 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2024_02_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing 9/15 ‚Äî Filing Date: 2023-11-14\n",
      "Accession: 0000950123-23-010898\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012323010898/0000950123-23-010898-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012323010898/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012323010898/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='27893.html'\n",
      "         ‚Üí Link text: '27893.html', href: '/Archives/edgar/data/1067983/000095012323010898/xslForm13F_X02/27893.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 27893.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012323010898/xslForm13F_X02/27893.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 155 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2023_11_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing 10/15 ‚Äî Filing Date: 2023-08-14\n",
      "Accession: 0000950123-23-008074\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012323008074/0000950123-23-008074-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012323008074/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012323008074/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='25376.html'\n",
      "         ‚Üí Link text: '25376.html', href: '/Archives/edgar/data/1067983/000095012323008074/xslForm13F_X02/25376.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 25376.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012323008074/xslForm13F_X02/25376.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 160 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2023_08_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing 11/15 ‚Äî Filing Date: 2023-05-15\n",
      "Accession: 0000950123-23-005270\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012323005270/0000950123-23-005270-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012323005270/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012323005270/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='22815.html'\n",
      "         ‚Üí Link text: '22815.html', href: '/Archives/edgar/data/1067983/000095012323005270/xslForm13F_X02/22815.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 22815.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012323005270/xslForm13F_X02/22815.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Extracted table with 168 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2023_05_15\n",
      "\n",
      "============================================================\n",
      "Processing 12/15 ‚Äî Filing Date: 2023-02-14\n",
      "Accession: 0000950123-23-002585\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012323002585/0000950123-23-002585-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012323002585/xslForm13F_X02/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012323002585/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='20651.html'\n",
      "         ‚Üí Link text: '20651.html', href: '/Archives/edgar/data/1067983/000095012323002585/xslForm13F_X02/20651.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 20651.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012323002585/xslForm13F_X02/20651.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 175 rows and 13 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2023_02_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing 13/15 ‚Äî Filing Date: 2022-11-14\n",
      "Accession: 0000950123-22-012275\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012322012275/0000950123-22-012275-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012322012275/xslForm13F_X01/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012322012275/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='18337.html'\n",
      "         ‚Üí Link text: '18337.html', href: '/Archives/edgar/data/1067983/000095012322012275/xslForm13F_X01/18337.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 18337.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012322012275/xslForm13F_X01/18337.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 182 rows and 12 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2022_11_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing 14/15 ‚Äî Filing Date: 2022-08-15\n",
      "Accession: 0000950123-22-009450\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012322009450/0000950123-22-009450-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012322009450/xslForm13F_X01/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012322009450/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='15796.html'\n",
      "         ‚Üí Link text: '15796.html', href: '/Archives/edgar/data/1067983/000095012322009450/xslForm13F_X01/15796.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 15796.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012322009450/xslForm13F_X01/15796.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 184 rows and 12 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2022_08_15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing 15/15 ‚Äî Filing Date: 2022-05-16\n",
      "Accession: 0000950123-22-006442\n",
      "============================================================\n",
      "   üìÑ Fetching: https://www.sec.gov/Archives/edgar/data/1067983/000095012322006442/0000950123-22-006442-index.htm\n",
      "   üîç Searching for INFORMATION TABLE .html document...\n",
      "   üêõ DEBUG: Showing ALL links in ALL rows:\n",
      "      Row 1: Type='13F-HR', Text='primary_doc.html'\n",
      "         ‚Üí Link text: 'primary_doc.html', href: '/Archives/edgar/data/1067983/000095012322006442/xslForm13F_X01/primary_doc.xml'\n",
      "      Row 2: Type='13F-HR', Text='primary_doc.xml'\n",
      "         ‚Üí Link text: 'primary_doc.xml', href: '/Archives/edgar/data/1067983/000095012322006442/primary_doc.xml'\n",
      "      Row 3: Type='INFORMATION TABLE', Text='13095.html'\n",
      "         ‚Üí Link text: '13095.html', href: '/Archives/edgar/data/1067983/000095012322006442/xslForm13F_X01/13095.xml'\n",
      "\n",
      "   ‚úÖ Found INFORMATION TABLE HTML: 13095.html\n",
      "   üîó URL: https://www.sec.gov/Archives/edgar/data/1067983/000095012322006442/xslForm13F_X01/13095.xml\n",
      "   üìä Scraping table from INFORMATION TABLE HTML...\n",
      "   ‚úÖ Extracted table with 179 rows and 12 columns\n",
      "   üìã Columns: [0, 1, 2, 3, 4]\n",
      "   ‚úÖ Data ready for sheet: 2022_05_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/21/13dv45411jx7lncnks5rdpqm0000gn/T/ipykernel_19495/1807052526.py:140: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(r.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìù Writing 15 sheets to Excel...\n",
      "============================================================\n",
      "   ‚úì Written sheet: 2025_11_14\n",
      "   ‚úì Written sheet: 2025_08_14\n",
      "   ‚úì Written sheet: 2025_05_15\n",
      "   ‚úì Written sheet: 2025_02_14\n",
      "   ‚úì Written sheet: 2024_11_14\n",
      "   ‚úì Written sheet: 2024_08_14\n",
      "   ‚úì Written sheet: 2024_05_15\n",
      "   ‚úì Written sheet: 2024_02_14\n",
      "   ‚úì Written sheet: 2023_11_14\n",
      "   ‚úì Written sheet: 2023_08_14\n",
      "   ‚úì Written sheet: 2023_05_15\n",
      "   ‚úì Written sheet: 2023_02_14\n",
      "   ‚úì Written sheet: 2022_11_14\n",
      "   ‚úì Written sheet: 2022_08_15\n",
      "   ‚úì Written sheet: 2022_05_16\n",
      "\n",
      "============================================================\n",
      "‚úÖ SUCCESS ‚Äî Processed 15/15 filings\n",
      "üìÅ Excel file created: 13F_Data.xlsx\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "CIK = \"0001067983\"\n",
    "BASE = \"https://www.sec.gov\"\n",
    "DATA_URL = f\"https://data.sec.gov/submissions/CIK{CIK}.json\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (DataResearchBot/1.0; your_email@example.com)\"\n",
    "}\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "\n",
    "# ==========================\n",
    "# 1. FETCH JSON INDEX\n",
    "# ==========================\n",
    "def fetch_filing_json():\n",
    "    print(\"1Ô∏è‚É£ Fetching SEC JSON index...\")\n",
    "    r = session.get(DATA_URL)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "# ==========================\n",
    "# 2. EXTRACT FIRST 15 VALID 13F-HR (NOT 13F-HR/A)\n",
    "# ==========================\n",
    "def extract_13f_filings(data, limit=15):\n",
    "    print(\"2Ô∏è‚É£ Filtering first 15 valid 13F-HR filings (excluding amendments)...\")\n",
    "    recent = data[\"filings\"][\"recent\"]\n",
    "    filings = []\n",
    "    \n",
    "    for form, acc, date in zip(\n",
    "        recent[\"form\"],\n",
    "        recent[\"accessionNumber\"],\n",
    "        recent[\"filingDate\"]\n",
    "    ):\n",
    "        # Only accept \"13F-HR\" exactly, NOT \"13F-HR/A\"\n",
    "        if form == \"13F-HR\":\n",
    "            filings.append((acc, date))\n",
    "            print(f\"   ‚úì {form} | {date} | {acc}\")\n",
    "            if len(filings) == limit:\n",
    "                break\n",
    "    \n",
    "    print(f\"\\n   Found {len(filings)} filings\")\n",
    "    return filings\n",
    "\n",
    "# ==========================\n",
    "# 3. GET INFORMATION TABLE HTML DOCUMENT LINK\n",
    "# ==========================\n",
    "def get_information_table_url(accession):\n",
    "    \"\"\"\n",
    "    Navigate to the filing's document page and find the INFORMATION TABLE .html file\n",
    "    ONLY .html files - NEVER .xml\n",
    "    \"\"\"\n",
    "    # Remove dashes for the URL path\n",
    "    acc_no_dash = accession.replace(\"-\", \"\")\n",
    "    \n",
    "    # Build the filing index page URL\n",
    "    filing_page = f\"{BASE}/Archives/edgar/data/{int(CIK)}/{acc_no_dash}/{accession}-index.htm\"\n",
    "    \n",
    "    print(f\"   üìÑ Fetching: {filing_page}\")\n",
    "    \n",
    "    try:\n",
    "        r = session.get(filing_page)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        # Find the document table\n",
    "        table = soup.find('table', {'class': 'tableFile'})\n",
    "        if not table:\n",
    "            # Try alternate table structure\n",
    "            table = soup.find('table', summary='Document Format Files')\n",
    "        \n",
    "        if not table:\n",
    "            print(\"   ‚ö†Ô∏è Document table not found\")\n",
    "            return None\n",
    "        \n",
    "        print(\"   üîç Searching for INFORMATION TABLE .html document...\")\n",
    "        print(\"   üêõ DEBUG: Showing ALL links in ALL rows:\")\n",
    "        \n",
    "        # Get all rows\n",
    "        all_rows = table.find_all('tr')\n",
    "        \n",
    "        for idx, row in enumerate(all_rows[1:], start=1):  # Skip header row\n",
    "            cells = row.find_all('td')\n",
    "            \n",
    "            if len(cells) >= 4:\n",
    "                # Get the ACTUAL TEXT from the document cell (column 2)\n",
    "                doc_cell_text = cells[2].get_text(strip=True)\n",
    "                \n",
    "                # Get type from column 3\n",
    "                doc_type = cells[3].get_text(strip=True)\n",
    "                \n",
    "                # Get ALL links in this cell\n",
    "                all_links = cells[2].find_all('a')\n",
    "                \n",
    "                print(f\"      Row {idx}: Type='{doc_type}', Text='{doc_cell_text}'\")\n",
    "                for link in all_links:\n",
    "                    href = link.get('href')\n",
    "                    link_text = link.get_text(strip=True)\n",
    "                    print(f\"         ‚Üí Link text: '{link_text}', href: '{href}'\")\n",
    "                \n",
    "                # Check if this is INFORMATION TABLE and the text is .html\n",
    "                if \"INFORMATION TABLE\" in doc_type.upper() and doc_cell_text.endswith('.html'):\n",
    "                    # Find the link that actually points to the .html file\n",
    "                    for link in all_links:\n",
    "                        href = link.get('href')\n",
    "                        link_text = link.get_text(strip=True)\n",
    "                        \n",
    "                        # Match the link text with our target filename\n",
    "                        if link_text == doc_cell_text:\n",
    "                            full_url = BASE + href if href.startswith('/') else href\n",
    "                            \n",
    "                            print(f\"\\n   ‚úÖ Found INFORMATION TABLE HTML: {doc_cell_text}\")\n",
    "                            print(f\"   üîó URL: {full_url}\")\n",
    "                            return full_url\n",
    "        \n",
    "        print(\"\\n   ‚ö†Ô∏è No INFORMATION TABLE .html file found\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# ==========================\n",
    "# 4. SCRAPE TABLE\n",
    "# ==========================\n",
    "def scrape_html_table(url):\n",
    "    print(f\"   üìä Scraping table from INFORMATION TABLE HTML...\")\n",
    "    try:\n",
    "        # Use the session with proper headers\n",
    "        r = session.get(url)\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        # Try pandas read_html with the HTML content\n",
    "        tables = pd.read_html(r.text)\n",
    "        \n",
    "        if tables:\n",
    "            # Find the table with the most rows (likely the holdings table)\n",
    "            main_table = max(tables, key=lambda x: len(x))\n",
    "            print(f\"   ‚úÖ Extracted table with {len(main_table)} rows and {len(main_table.columns)} columns\")\n",
    "            \n",
    "            # Show first few column names for verification\n",
    "            if len(main_table.columns) > 0:\n",
    "                print(f\"   üìã Columns: {list(main_table.columns[:5])}\")\n",
    "            \n",
    "            return main_table\n",
    "        \n",
    "        print(\"   ‚ö†Ô∏è No tables found with pandas\")\n",
    "        return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error scraping: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# ==========================\n",
    "# 5. MAIN PIPELINE\n",
    "# ==========================\n",
    "def main():\n",
    "    try:\n",
    "        data = fetch_filing_json()\n",
    "        filings = extract_13f_filings(data, limit=15)\n",
    "        \n",
    "        if not filings:\n",
    "            print(\"‚ùå No filings found!\")\n",
    "            return\n",
    "        \n",
    "        # Collect all successful dataframes first\n",
    "        all_data = []\n",
    "        \n",
    "        for i, (accession, filing_date) in enumerate(filings, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Processing {i}/{len(filings)} ‚Äî Filing Date: {filing_date}\")\n",
    "            print(f\"Accession: {accession}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            # Get the INFORMATION TABLE HTML URL\n",
    "            html_url = get_information_table_url(accession)\n",
    "            \n",
    "            if not html_url:\n",
    "                print(\"   ‚ö†Ô∏è Skipping ‚Äî No INFORMATION TABLE .html found\")\n",
    "                continue\n",
    "            \n",
    "            # Scrape the table data\n",
    "            df = scrape_html_table(html_url)\n",
    "            \n",
    "            if df is not None and not df.empty:\n",
    "                # Create sheet name from filing date\n",
    "                sheet = filing_date.replace(\"-\", \"_\")[:31]\n",
    "                all_data.append((sheet, df))\n",
    "                print(f\"   ‚úÖ Data ready for sheet: {sheet}\")\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è Skipping ‚Äî No data extracted\")\n",
    "            \n",
    "            # Be respectful to SEC servers\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Now write to Excel only if we have data\n",
    "        if all_data:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üìù Writing {len(all_data)} sheets to Excel...\")\n",
    "            print('='*60)\n",
    "            \n",
    "            with pd.ExcelWriter(\"13F_Data.xlsx\", engine=\"openpyxl\") as writer:\n",
    "                for sheet_name, df in all_data:\n",
    "                    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                    print(f\"   ‚úì Written sheet: {sheet_name}\")\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"‚úÖ SUCCESS ‚Äî Processed {len(all_data)}/{len(filings)} filings\")\n",
    "            print(f\"üìÅ Excel file created: 13F_Data.xlsx\")\n",
    "            print('='*60)\n",
    "        else:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"‚ùå No data was extracted from any filing\")\n",
    "            print(\"Excel file NOT created\")\n",
    "            print('='*60)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå FATAL ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# ==========================\n",
    "# RUN\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bf0fa6-7176-41fe-afd2-809a484c2be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
